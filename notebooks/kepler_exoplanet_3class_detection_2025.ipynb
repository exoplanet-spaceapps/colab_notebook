{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Kepler ç³»å¤–è¡Œæ˜Ÿä¸‰åˆ†ç±»æ£€æµ‹ç³»ç»Ÿ (2025)\n",
    "\n",
    "## é¡¹ç›®æ¦‚è¿°\n",
    "æœ¬ Notebook å®ç°äº†åŸºäº NASA Kepler ä»»åŠ¡æ•°æ®çš„ç³»å¤–è¡Œæ˜Ÿä¸‰åˆ†ç±»æ£€æµ‹ç³»ç»Ÿï¼š\n",
    "- **CONFIRMED**: å·²ç¡®è®¤çš„ç³»å¤–è¡Œæ˜Ÿ\n",
    "- **CANDIDATE**: å€™é€‰ç³»å¤–è¡Œæ˜Ÿ\n",
    "- **FALSE POSITIVE**: å‡é˜³æ€§\n",
    "\n",
    "## æ¨¡å‹æ¶æ„\n",
    "1. Genesis CNN (æ·±åº¦å­¦ä¹ )\n",
    "2. XGBoost (æ¢¯åº¦æå‡)\n",
    "3. Random Forest (éšæœºæ£®æ—)\n",
    "4. Voting Ensemble (é›†æˆå­¦ä¹ )\n",
    "\n",
    "## è¿è¡Œç¯å¢ƒ\n",
    "- Google Colab (2025å¹´10æœˆå…¼å®¹)\n",
    "- TensorFlow 2.15.0\n",
    "- Python 3.10+\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1"
   },
   "source": [
    "## 1. ç¯å¢ƒè®¾ç½®ä¸ä¾èµ–å®‰è£…\n",
    "\n",
    "å®‰è£…æ‰€æœ‰å¿…éœ€çš„åº“ï¼Œç¡®ä¿ä¸ 2025 å¹´ 10 æœˆç¯å¢ƒå…¼å®¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–åº“ï¼ˆ2025å¹´10æœˆå…¼å®¹ç‰ˆæœ¬ï¼‰\n",
    "!pip install -q tensorflow==2.15.0 xgboost==2.0.3 scikit-learn==1.3.2 \\\n",
    "    pandas==2.1.4 numpy==1.24.3 matplotlib==3.8.2 seaborn==0.13.0 \\\n",
    "    imbalanced-learn==0.11.0 joblib==1.3.2\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰ä¾èµ–åº“å®‰è£…å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_curve, auc,\n",
    "    accuracy_score, precision_recall_fscore_support\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import joblib\n",
    "from google.colab import files\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾æ ·å¼\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"TensorFlow ç‰ˆæœ¬: {tf.__version__}\")\n",
    "print(f\"XGBoost ç‰ˆæœ¬: {xgb.__version__}\")\n",
    "print(f\"âœ… æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2"
   },
   "source": [
    "## 2. æ•°æ®ä¸Šä¼ ä¸åŠ è½½\n",
    "\n",
    "ä¸Šä¼  Kepler æ•°æ®é›† CSV æ–‡ä»¶å¹¶è¿›è¡Œåˆæ­¥æ£€æŸ¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_data"
   },
   "outputs": [],
   "source": [
    "# ä¸Šä¼ æ•°æ®æ–‡ä»¶\n",
    "print(\"ğŸ“¤ è¯·ä¸Šä¼  Kepler æ•°æ®é›† CSV æ–‡ä»¶...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# è·å–ä¸Šä¼ çš„æ–‡ä»¶å\n",
    "csv_filename = list(uploaded.keys())[0]\n",
    "print(f\"âœ… æ–‡ä»¶å·²ä¸Šä¼ : {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "print(f\"æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\n",
    "print(f\"\\nåˆ—å: {df.columns.tolist()}\")\n",
    "print(f\"\\nå‰ 5 è¡Œæ•°æ®:\")\n",
    "display(df.head())\n",
    "\n",
    "# æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ\n",
    "if 'koi_disposition' in df.columns:\n",
    "    print(f\"\\næ ‡ç­¾åˆ†å¸ƒ:\")\n",
    "    print(df['koi_disposition'].value_counts())\n",
    "    print(f\"\\næ ‡ç­¾åˆ†å¸ƒæ¯”ä¾‹:\")\n",
    "    print(df['koi_disposition'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section3"
   },
   "source": [
    "## 3. æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "æ¸…æ´—æ•°æ®ã€å¤„ç†ç¼ºå¤±å€¼ã€ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®å¹³è¡¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocessing_functions"
   },
   "outputs": [],
   "source": [
    "def preprocess_kepler_data(df, test_size=0.2, balance_data=True):\n",
    "    \"\"\"\n",
    "    é¢„å¤„ç† Kepler æ•°æ®é›†\n",
    "    \n",
    "    å‚æ•°:\n",
    "        df: åŸå§‹æ•°æ®æ¡†\n",
    "        test_size: æµ‹è¯•é›†æ¯”ä¾‹\n",
    "        balance_data: æ˜¯å¦ä½¿ç”¨ SMOTE å¹³è¡¡æ•°æ®\n",
    "    \n",
    "    è¿”å›:\n",
    "        X_train, X_test, y_train, y_test, label_encoder, scaler\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†...\")\n",
    "    \n",
    "    # 1. å¤åˆ¶æ•°æ®\n",
    "    data = df.copy()\n",
    "    \n",
    "    # 2. å¤„ç†æ ‡ç­¾åˆ—\n",
    "    label_col = 'koi_disposition'\n",
    "    if label_col not in data.columns:\n",
    "        raise ValueError(f\"æ•°æ®é›†ä¸­æœªæ‰¾åˆ°æ ‡ç­¾åˆ— '{label_col}'\")\n",
    "    \n",
    "    # 3. åˆ é™¤ä¸ç›¸å…³çš„åˆ—\n",
    "    cols_to_drop = ['kepid', 'kepoi_name', 'kepler_name', 'koi_tce_delivname']\n",
    "    cols_to_drop = [col for col in cols_to_drop if col in data.columns]\n",
    "    data = data.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # 4. åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾\n",
    "    X = data.drop(columns=[label_col])\n",
    "    y = data[label_col]\n",
    "    \n",
    "    # 5. å¤„ç†ç¼ºå¤±å€¼\n",
    "    print(f\"ç¼ºå¤±å€¼æ•°é‡: {X.isnull().sum().sum()}\")\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    # 6. æ ‡ç­¾ç¼–ç \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    print(f\"\\næ ‡ç­¾æ˜ å°„: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "    \n",
    "    # 7. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=test_size, random_state=RANDOM_SEED, stratify=y_encoded\n",
    "    )\n",
    "    print(f\"\\nè®­ç»ƒé›†å¤§å°: {X_train.shape}\")\n",
    "    print(f\"æµ‹è¯•é›†å¤§å°: {X_test.shape}\")\n",
    "    \n",
    "    # 8. ç‰¹å¾æ ‡å‡†åŒ–\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 9. æ•°æ®å¹³è¡¡ï¼ˆSMOTEï¼‰\n",
    "    if balance_data:\n",
    "        print(\"\\nğŸ”„ ä½¿ç”¨ SMOTE è¿›è¡Œæ•°æ®å¹³è¡¡...\")\n",
    "        smote = SMOTE(random_state=RANDOM_SEED)\n",
    "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "        print(f\"å¹³è¡¡åè®­ç»ƒé›†å¤§å°: {X_train_balanced.shape}\")\n",
    "        print(f\"å¹³è¡¡åæ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_train_balanced)}\")\n",
    "        return X_train_balanced, X_test_scaled, y_train_balanced, y_test, label_encoder, scaler\n",
    "    else:\n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test, label_encoder, scaler\n",
    "\n",
    "# æ‰§è¡Œé¢„å¤„ç†\n",
    "X_train, X_test, y_train, y_test, label_encoder, scaler = preprocess_kepler_data(df)\n",
    "\n",
    "print(f\"\\nâœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼\")\n",
    "print(f\"ç‰¹å¾æ•°é‡: {X_train.shape[1]}\")\n",
    "print(f\"ç±»åˆ«æ•°é‡: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section4"
   },
   "source": [
    "## 4. æ¨¡å‹å®šä¹‰\n",
    "\n",
    "å®šä¹‰æ‰€æœ‰åˆ†ç±»æ¨¡å‹æ¶æ„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "genesis_cnn"
   },
   "outputs": [],
   "source": [
    "def build_genesis_cnn(input_dim, num_classes=3):\n",
    "    \"\"\"\n",
    "    æ„å»º Genesis CNN æ¨¡å‹\n",
    "    \n",
    "    æ¶æ„:\n",
    "        - è¾“å…¥å±‚\n",
    "        - 3ä¸ªå·ç§¯å—ï¼ˆConv1D + BatchNorm + ReLU + Dropoutï¼‰\n",
    "        - å…¨å±€å¹³å‡æ± åŒ–\n",
    "        - å…¨è¿æ¥å±‚\n",
    "        - Softmax è¾“å‡ºå±‚\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(input_dim, 1), name='input')\n",
    "    \n",
    "    # å·ç§¯å— 1\n",
    "    x = layers.Conv1D(64, 3, padding='same', name='conv1')(inputs)\n",
    "    x = layers.BatchNormalization(name='bn1')(x)\n",
    "    x = layers.Activation('relu', name='relu1')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout1')(x)\n",
    "    \n",
    "    # å·ç§¯å— 2\n",
    "    x = layers.Conv1D(128, 3, padding='same', name='conv2')(x)\n",
    "    x = layers.BatchNormalization(name='bn2')(x)\n",
    "    x = layers.Activation('relu', name='relu2')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout2')(x)\n",
    "    \n",
    "    # å·ç§¯å— 3\n",
    "    x = layers.Conv1D(256, 3, padding='same', name='conv3')(x)\n",
    "    x = layers.BatchNormalization(name='bn3')(x)\n",
    "    x = layers.Activation('relu', name='relu3')(x)\n",
    "    x = layers.Dropout(0.4, name='dropout3')(x)\n",
    "    \n",
    "    # å…¨å±€æ± åŒ–\n",
    "    x = layers.GlobalAveragePooling1D(name='global_pool')(x)\n",
    "    \n",
    "    # å…¨è¿æ¥å±‚\n",
    "    x = layers.Dense(128, activation='relu', name='dense1')(x)\n",
    "    x = layers.Dropout(0.5, name='dropout4')(x)\n",
    "    \n",
    "    # è¾“å‡ºå±‚\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Genesis_CNN')\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# æ„å»ºæ¨¡å‹\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "genesis_cnn = build_genesis_cnn(num_features, num_classes)\n",
    "print(genesis_cnn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section5"
   },
   "source": [
    "## 5. æ¨¡å‹è®­ç»ƒ\n",
    "\n",
    "è®­ç»ƒæ‰€æœ‰æ¨¡å‹å¹¶ä¿å­˜æœ€ä½³ç‰ˆæœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_genesis"
   },
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# å‡†å¤‡æ•°æ®ï¼ˆCNNéœ€è¦3Dè¾“å…¥ï¼‰\n",
    "X_train_cnn = X_train.reshape(-1, num_features, 1)\n",
    "X_test_cnn = X_test.reshape(-1, num_features, 1)\n",
    "\n",
    "# è®¾ç½®å›è°ƒå‡½æ•°\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1),\n",
    "    ModelCheckpoint('models/genesis_cnn_best.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "]\n",
    "\n",
    "# è®­ç»ƒ Genesis CNN\n",
    "print(\"ğŸš€ å¼€å§‹è®­ç»ƒ Genesis CNN...\")\n",
    "history_cnn = genesis_cnn.fit(\n",
    "    X_train_cnn, y_train,\n",
    "    validation_data=(X_test_cnn, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "genesis_cnn.save('models/genesis_cnn_final.keras')\n",
    "print(\"âœ… Genesis CNN è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_xgboost"
   },
   "outputs": [],
   "source": [
    "# è®­ç»ƒ XGBoost\n",
    "print(\"\\nğŸš€ å¼€å§‹è®­ç»ƒ XGBoost...\")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='multi:softmax',\n",
    "    num_class=num_classes,\n",
    "    random_state=RANDOM_SEED,\n",
    "    eval_metric='mlogloss',\n",
    "    early_stopping_rounds=15\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "joblib.dump(xgb_model, 'models/xgboost_model.pkl')\n",
    "print(\"âœ… XGBoost è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_rf"
   },
   "outputs": [],
   "source": [
    "# è®­ç»ƒ Random Forest\n",
    "print(\"\\nğŸš€ å¼€å§‹è®­ç»ƒ Random Forest...\")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "joblib.dump(rf_model, 'models/random_forest_model.pkl')\n",
    "print(\"âœ… Random Forest è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_ensemble"
   },
   "outputs": [],
   "source": [
    "# è®­ç»ƒ Voting Ensemble\n",
    "print(\"\\nğŸš€ å¼€å§‹è®­ç»ƒ Voting Ensemble...\")\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªåŒ…è£…å™¨ç”¨äº Genesis CNN\n",
    "class CNNWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_reshaped = X.reshape(-1, X.shape[1], 1)\n",
    "        return np.argmax(self.model.predict(X_reshaped, verbose=0), axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_reshaped = X.reshape(-1, X.shape[1], 1)\n",
    "        return self.model.predict(X_reshaped, verbose=0)\n",
    "\n",
    "cnn_wrapper = CNNWrapper(genesis_cnn)\n",
    "\n",
    "# åˆ›å»ºé›†æˆæ¨¡å‹\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('genesis_cnn', cnn_wrapper),\n",
    "        ('xgboost', xgb_model),\n",
    "        ('random_forest', rf_model)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# ä¿å­˜é›†æˆæ¨¡å‹\n",
    "joblib.dump(voting_clf, 'models/voting_ensemble_model.pkl')\n",
    "print(\"âœ… Voting Ensemble è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section6"
   },
   "source": [
    "## 6. æ¨¡å‹è¯„ä¼°ä¸å¯è§†åŒ–\n",
    "\n",
    "è¯„ä¼°æ‰€æœ‰æ¨¡å‹æ€§èƒ½å¹¶è¿›è¡Œå¯è§†åŒ–æ¯”è¾ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation_functions"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name, is_cnn=False):\n",
    "    \"\"\"\n",
    "    è¯„ä¼°å•ä¸ªæ¨¡å‹\n",
    "    \n",
    "    è¿”å›:\n",
    "        metrics: åŒ…å«å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"è¯„ä¼°æ¨¡å‹: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # é¢„æµ‹\n",
    "    if is_cnn:\n",
    "        X_test_input = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "        y_pred_proba = model.predict(X_test_input, verbose=0)\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_test)\n",
    "        else:\n",
    "            y_pred_proba = None\n",
    "    \n",
    "    # è®¡ç®—æŒ‡æ ‡\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\nğŸ“Š æ•´ä½“æ€§èƒ½:\")\n",
    "    print(f\"  å‡†ç¡®ç‡: {accuracy:.4f}\")\n",
    "    print(f\"  ç²¾ç¡®ç‡: {precision:.4f}\")\n",
    "    print(f\"  å¬å›ç‡: {recall:.4f}\")\n",
    "    print(f\"  F1åˆ†æ•°: {f1:.4f}\")\n",
    "    \n",
    "    # åˆ†ç±»æŠ¥å‘Š\n",
    "    print(f\"\\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "    \n",
    "    # æ··æ·†çŸ©é˜µ\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(cm, model_name, class_names):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶æ··æ·†çŸ©é˜µ\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'æ··æ·†çŸ©é˜µ - {model_name}')\n",
    "    plt.ylabel('çœŸå®æ ‡ç­¾')\n",
    "    plt.xlabel('é¢„æµ‹æ ‡ç­¾')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'models/{model_name}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curves(models_metrics, X_test, y_test, class_names):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶å¤šç±»åˆ« ROC æ›²çº¿ï¼ˆOne-vs-Restï¼‰\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from itertools import cycle\n",
    "    \n",
    "    # äºŒå€¼åŒ–æ ‡ç­¾\n",
    "    y_test_bin = label_binarize(y_test, classes=range(len(class_names)))\n",
    "    n_classes = len(class_names)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = cycle(['blue', 'red', 'green'])\n",
    "    \n",
    "    for idx, metrics in enumerate(models_metrics):\n",
    "        if metrics['y_pred_proba'] is not None:\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            for i, color, class_name in zip(range(n_classes), colors, class_names):\n",
    "                fpr, tpr, _ = roc_curve(y_test_bin[:, i], metrics['y_pred_proba'][:, i])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                \n",
    "                ax.plot(fpr, tpr, color=color, lw=2,\n",
    "                       label=f'{class_name} (AUC = {roc_auc:.2f})')\n",
    "            \n",
    "            ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "            ax.set_xlim([0.0, 1.0])\n",
    "            ax.set_ylim([0.0, 1.05])\n",
    "            ax.set_xlabel('å‡é˜³æ€§ç‡')\n",
    "            ax.set_ylabel('çœŸé˜³æ€§ç‡')\n",
    "            ax.set_title(f'ROC æ›²çº¿ - {metrics[\"model_name\"]}')\n",
    "            ax.legend(loc='lower right')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_all"
   },
   "outputs": [],
   "source": [
    "# è¯„ä¼°æ‰€æœ‰æ¨¡å‹\n",
    "all_metrics = []\n",
    "\n",
    "# Genesis CNN\n",
    "metrics_cnn = evaluate_model(genesis_cnn, X_test, y_test, 'Genesis CNN', is_cnn=True)\n",
    "all_metrics.append(metrics_cnn)\n",
    "plot_confusion_matrix(metrics_cnn['confusion_matrix'], 'Genesis_CNN', label_encoder.classes_)\n",
    "\n",
    "# XGBoost\n",
    "metrics_xgb = evaluate_model(xgb_model, X_test, y_test, 'XGBoost')\n",
    "all_metrics.append(metrics_xgb)\n",
    "plot_confusion_matrix(metrics_xgb['confusion_matrix'], 'XGBoost', label_encoder.classes_)\n",
    "\n",
    "# Random Forest\n",
    "metrics_rf = evaluate_model(rf_model, X_test, y_test, 'Random Forest')\n",
    "all_metrics.append(metrics_rf)\n",
    "plot_confusion_matrix(metrics_rf['confusion_matrix'], 'Random_Forest', label_encoder.classes_)\n",
    "\n",
    "# Voting Ensemble\n",
    "metrics_ensemble = evaluate_model(voting_clf, X_test, y_test, 'Voting Ensemble')\n",
    "all_metrics.append(metrics_ensemble)\n",
    "plot_confusion_matrix(metrics_ensemble['confusion_matrix'], 'Voting_Ensemble', label_encoder.classes_)\n",
    "\n",
    "# ç»˜åˆ¶ ROC æ›²çº¿å¯¹æ¯”\n",
    "plot_roc_curves(all_metrics, X_test, y_test, label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare_models"
   },
   "outputs": [],
   "source": [
    "# æ¨¡å‹æ€§èƒ½å¯¹æ¯”\n",
    "comparison_df = pd.DataFrame([{\n",
    "    'æ¨¡å‹': m['model_name'],\n",
    "    'å‡†ç¡®ç‡': f\"{m['accuracy']:.4f}\",\n",
    "    'ç²¾ç¡®ç‡': f\"{m['precision']:.4f}\",\n",
    "    'å¬å›ç‡': f\"{m['recall']:.4f}\",\n",
    "    'F1åˆ†æ•°': f\"{m['f1']:.4f}\"\n",
    "} for m in all_metrics])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df)\n",
    "\n",
    "# å¯è§†åŒ–å¯¹æ¯”\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "metric_names = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°']\n",
    "\n",
    "for idx, (metric, name) in enumerate(zip(metrics_to_plot, metric_names)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    values = [m[metric] for m in all_metrics]\n",
    "    model_names = [m['model_name'] for m in all_metrics]\n",
    "    \n",
    "    bars = ax.bar(model_names, values, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])\n",
    "    ax.set_ylabel(name)\n",
    "    ax.set_title(f'{name}å¯¹æ¯”')\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('models/models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section7"
   },
   "source": [
    "## 7. è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹\n",
    "\n",
    "åŸºäºç»¼åˆè¯„åˆ†è‡ªåŠ¨é€‰æ‹©æ€§èƒ½æœ€å¥½çš„æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "select_best"
   },
   "outputs": [],
   "source": [
    "def select_best_model(metrics_list, weights={'accuracy': 0.3, 'precision': 0.2, 'recall': 0.2, 'f1': 0.3}):\n",
    "    \"\"\"\n",
    "    åŸºäºåŠ æƒè¯„åˆ†é€‰æ‹©æœ€ä½³æ¨¡å‹\n",
    "    \n",
    "    å‚æ•°:\n",
    "        metrics_list: æ‰€æœ‰æ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨\n",
    "        weights: å„æŒ‡æ ‡çš„æƒé‡\n",
    "    \n",
    "    è¿”å›:\n",
    "        best_model_name, best_metrics\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for metrics in metrics_list:\n",
    "        score = (\n",
    "            weights['accuracy'] * metrics['accuracy'] +\n",
    "            weights['precision'] * metrics['precision'] +\n",
    "            weights['recall'] * metrics['recall'] +\n",
    "            weights['f1'] * metrics['f1']\n",
    "        )\n",
    "        scores.append(score)\n",
    "    \n",
    "    best_idx = np.argmax(scores)\n",
    "    best_metrics = metrics_list[best_idx]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ† æœ€ä½³æ¨¡å‹é€‰æ‹©\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for idx, (metrics, score) in enumerate(zip(metrics_list, scores)):\n",
    "        marker = \"ğŸ‘‘\" if idx == best_idx else \"  \"\n",
    "        print(f\"{marker} {metrics['model_name']}: ç»¼åˆå¾—åˆ† = {score:.4f}\")\n",
    "    \n",
    "    print(f\"\\nâœ¨ æœ€ä½³æ¨¡å‹: {best_metrics['model_name']}\")\n",
    "    print(f\"   å‡†ç¡®ç‡: {best_metrics['accuracy']:.4f}\")\n",
    "    print(f\"   ç²¾ç¡®ç‡: {best_metrics['precision']:.4f}\")\n",
    "    print(f\"   å¬å›ç‡: {best_metrics['recall']:.4f}\")\n",
    "    print(f\"   F1åˆ†æ•°: {best_metrics['f1']:.4f}\")\n",
    "    \n",
    "    return best_metrics['model_name'], best_metrics\n",
    "\n",
    "# é€‰æ‹©æœ€ä½³æ¨¡å‹\n",
    "best_model_name, best_metrics = select_best_model(all_metrics)\n",
    "\n",
    "# ä¿å­˜æœ€ä½³æ¨¡å‹ä¿¡æ¯\n",
    "best_model_info = {\n",
    "    'model_name': best_model_name,\n",
    "    'metrics': {\n",
    "        'accuracy': float(best_metrics['accuracy']),\n",
    "        'precision': float(best_metrics['precision']),\n",
    "        'recall': float(best_metrics['recall']),\n",
    "        'f1': float(best_metrics['f1'])\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'num_classes': num_classes,\n",
    "    'class_names': label_encoder.classes_.tolist()\n",
    "}\n",
    "\n",
    "with open('models/best_model_info.json', 'w') as f:\n",
    "    json.dump(best_model_info, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… æœ€ä½³æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section8"
   },
   "source": [
    "## 8. è®­ç»ƒå†å²å¯è§†åŒ–ï¼ˆCNNï¼‰\n",
    "\n",
    "å¯è§†åŒ– Genesis CNN çš„è®­ç»ƒè¿‡ç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_history"
   },
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶è®­ç»ƒå†å²\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# æŸå¤±æ›²çº¿\n",
    "axes[0].plot(history_cnn.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)\n",
    "axes[0].plot(history_cnn.history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('æŸå¤±')\n",
    "axes[0].set_title('Genesis CNN è®­ç»ƒæŸå¤±æ›²çº¿')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# å‡†ç¡®ç‡æ›²çº¿\n",
    "axes[1].plot(history_cnn.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)\n",
    "axes[1].plot(history_cnn.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('å‡†ç¡®ç‡')\n",
    "axes[1].set_title('Genesis CNN è®­ç»ƒå‡†ç¡®ç‡æ›²çº¿')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('models/genesis_cnn_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section9"
   },
   "source": [
    "## 9. ä¿å­˜æ¨¡å‹å…ƒæ•°æ®\n",
    "\n",
    "ä¿å­˜æ‰€æœ‰å¿…è¦çš„é¢„å¤„ç†å™¨å’Œæ¨¡å‹é…ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_metadata"
   },
   "outputs": [],
   "source": [
    "# ä¿å­˜é¢„å¤„ç†å™¨\n",
    "joblib.dump(scaler, 'models/scaler.pkl')\n",
    "joblib.dump(label_encoder, 'models/label_encoder.pkl')\n",
    "\n",
    "# ä¿å­˜æ‰€æœ‰æ¨¡å‹çš„å…ƒæ•°æ®\n",
    "metadata = {\n",
    "    'project': 'Kepler Exoplanet 3-Class Detection',\n",
    "    'version': '2.0',\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'num_features': num_features,\n",
    "    'num_classes': num_classes,\n",
    "    'class_names': label_encoder.classes_.tolist(),\n",
    "    'train_size': len(X_train),\n",
    "    'test_size': len(X_test),\n",
    "    'models': {\n",
    "        'genesis_cnn': {\n",
    "            'type': 'deep_learning',\n",
    "            'framework': 'tensorflow',\n",
    "            'file': 'genesis_cnn_final.keras',\n",
    "            'metrics': {\n",
    "                'accuracy': float(metrics_cnn['accuracy']),\n",
    "                'precision': float(metrics_cnn['precision']),\n",
    "                'recall': float(metrics_cnn['recall']),\n",
    "                'f1': float(metrics_cnn['f1'])\n",
    "            }\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'type': 'gradient_boosting',\n",
    "            'framework': 'xgboost',\n",
    "            'file': 'xgboost_model.pkl',\n",
    "            'metrics': {\n",
    "                'accuracy': float(metrics_xgb['accuracy']),\n",
    "                'precision': float(metrics_xgb['precision']),\n",
    "                'recall': float(metrics_xgb['recall']),\n",
    "                'f1': float(metrics_xgb['f1'])\n",
    "            }\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'type': 'ensemble',\n",
    "            'framework': 'sklearn',\n",
    "            'file': 'random_forest_model.pkl',\n",
    "            'metrics': {\n",
    "                'accuracy': float(metrics_rf['accuracy']),\n",
    "                'precision': float(metrics_rf['precision']),\n",
    "                'recall': float(metrics_rf['recall']),\n",
    "                'f1': float(metrics_rf['f1'])\n",
    "            }\n",
    "        },\n",
    "        'voting_ensemble': {\n",
    "            'type': 'ensemble',\n",
    "            'framework': 'sklearn',\n",
    "            'file': 'voting_ensemble_model.pkl',\n",
    "            'metrics': {\n",
    "                'accuracy': float(metrics_ensemble['accuracy']),\n",
    "                'precision': float(metrics_ensemble['precision']),\n",
    "                'recall': float(metrics_ensemble['recall']),\n",
    "                'f1': float(metrics_ensemble['f1'])\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'best_model': best_model_name,\n",
    "    'preprocessing': {\n",
    "        'scaler': 'StandardScaler',\n",
    "        'label_encoder': 'LabelEncoder',\n",
    "        'balance_method': 'SMOTE'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('models/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"âœ… å…ƒæ•°æ®å·²ä¿å­˜ï¼\")\n",
    "print(\"\\nğŸ“¦ æ¨¡å‹æ–‡ä»¶åˆ—è¡¨:\")\n",
    "for file in os.listdir('models'):\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section10"
   },
   "source": [
    "## 10. æ‰“åŒ…å¹¶ä¸‹è½½æ¨¡å‹\n",
    "\n",
    "å°†æ‰€æœ‰æ¨¡å‹æ–‡ä»¶æ‰“åŒ…æˆ ZIP æ–‡ä»¶ä¾›ä¸‹è½½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "package_models"
   },
   "outputs": [],
   "source": [
    "# åˆ›å»º ZIP æ–‡ä»¶\n",
    "zip_filename = 'kepler_exoplanet_models_2025.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk('models'):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(file_path, '.')\n",
    "            zipf.write(file_path, arcname)\n",
    "            print(f\"âœ… å·²æ·»åŠ : {arcname}\")\n",
    "\n",
    "print(f\"\\nğŸ“¦ æ‰€æœ‰æ¨¡å‹å·²æ‰“åŒ…åˆ°: {zip_filename}\")\n",
    "print(f\"æ–‡ä»¶å¤§å°: {os.path.getsize(zip_filename) / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_models"
   },
   "outputs": [],
   "source": [
    "# ä¸‹è½½ ZIP æ–‡ä»¶\n",
    "print(\"ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹æ–‡ä»¶...\")\n",
    "files.download(zip_filename)\n",
    "print(\"âœ… ä¸‹è½½å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section11"
   },
   "source": [
    "## 11. ä½¿ç”¨ç¤ºä¾‹\n",
    "\n",
    "æ¼”ç¤ºå¦‚ä½•åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usage_example"
   },
   "outputs": [],
   "source": [
    "# ç¤ºä¾‹ï¼šåŠ è½½æœ€ä½³æ¨¡å‹å¹¶é¢„æµ‹\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”® é¢„æµ‹ç¤ºä¾‹\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# åŠ è½½å…ƒæ•°æ®\n",
    "with open('models/metadata.json', 'r') as f:\n",
    "    loaded_metadata = json.load(f)\n",
    "\n",
    "print(f\"\\næœ€ä½³æ¨¡å‹: {loaded_metadata['best_model']}\")\n",
    "\n",
    "# åŠ è½½é¢„å¤„ç†å™¨\n",
    "loaded_scaler = joblib.load('models/scaler.pkl')\n",
    "loaded_label_encoder = joblib.load('models/label_encoder.pkl')\n",
    "\n",
    "# é€‰æ‹©å‡ ä¸ªæµ‹è¯•æ ·æœ¬\n",
    "sample_indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "X_sample = X_test[sample_indices]\n",
    "y_sample_true = y_test[sample_indices]\n",
    "\n",
    "# æ ¹æ®æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹\n",
    "if best_model_name == 'Genesis CNN':\n",
    "    X_sample_input = X_sample.reshape(-1, num_features, 1)\n",
    "    y_sample_pred_proba = genesis_cnn.predict(X_sample_input, verbose=0)\n",
    "    y_sample_pred = np.argmax(y_sample_pred_proba, axis=1)\n",
    "elif best_model_name == 'XGBoost':\n",
    "    y_sample_pred = xgb_model.predict(X_sample)\n",
    "    y_sample_pred_proba = xgb_model.predict_proba(X_sample)\n",
    "elif best_model_name == 'Random Forest':\n",
    "    y_sample_pred = rf_model.predict(X_sample)\n",
    "    y_sample_pred_proba = rf_model.predict_proba(X_sample)\n",
    "else:  # Voting Ensemble\n",
    "    y_sample_pred = voting_clf.predict(X_sample)\n",
    "    y_sample_pred_proba = voting_clf.predict_proba(X_sample)\n",
    "\n",
    "# æ˜¾ç¤ºé¢„æµ‹ç»“æœ\n",
    "print(\"\\né¢„æµ‹ç»“æœ:\")\n",
    "for i in range(len(sample_indices)):\n",
    "    true_label = loaded_label_encoder.classes_[y_sample_true[i]]\n",
    "    pred_label = loaded_label_encoder.classes_[y_sample_pred[i]]\n",
    "    confidence = y_sample_pred_proba[i][y_sample_pred[i]] * 100\n",
    "    \n",
    "    match = \"âœ“\" if y_sample_true[i] == y_sample_pred[i] else \"âœ—\"\n",
    "    print(f\"\\næ ·æœ¬ {i+1}: {match}\")\n",
    "    print(f\"  çœŸå®æ ‡ç­¾: {true_label}\")\n",
    "    print(f\"  é¢„æµ‹æ ‡ç­¾: {pred_label}\")\n",
    "    print(f\"  ç½®ä¿¡åº¦: {confidence:.2f}%\")\n",
    "    print(f\"  æ¦‚ç‡åˆ†å¸ƒ: {dict(zip(loaded_label_encoder.classes_, [f'{p*100:.2f}%' for p in y_sample_pred_proba[i]]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "### ğŸ“Š é¡¹ç›®æˆæœ\n",
    "\n",
    "æœ¬ Notebook æˆåŠŸå®ç°äº† Kepler ç³»å¤–è¡Œæ˜Ÿä¸‰åˆ†ç±»æ£€æµ‹ç³»ç»Ÿï¼ŒåŒ…æ‹¬ï¼š\n",
    "\n",
    "1. **æ•°æ®å¤„ç†**\n",
    "   - è‡ªåŠ¨æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹\n",
    "   - SMOTE æ•°æ®å¹³è¡¡\n",
    "   - æ ‡å‡†åŒ–é¢„å¤„ç†\n",
    "\n",
    "2. **æ¨¡å‹è®­ç»ƒ**\n",
    "   - Genesis CNNï¼ˆæ·±åº¦å­¦ä¹ ï¼‰\n",
    "   - XGBoostï¼ˆæ¢¯åº¦æå‡ï¼‰\n",
    "   - Random Forestï¼ˆéšæœºæ£®æ—ï¼‰\n",
    "   - Voting Ensembleï¼ˆé›†æˆå­¦ä¹ ï¼‰\n",
    "\n",
    "3. **æ€§èƒ½è¯„ä¼°**\n",
    "   - æ··æ·†çŸ©é˜µå¯è§†åŒ–\n",
    "   - ROC æ›²çº¿åˆ†æ\n",
    "   - ç»¼åˆæŒ‡æ ‡å¯¹æ¯”\n",
    "   - è‡ªåŠ¨æœ€ä½³æ¨¡å‹é€‰æ‹©\n",
    "\n",
    "4. **å¯éƒ¨ç½²èµ„æº**\n",
    "   - æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "   - é¢„å¤„ç†å™¨å’Œç¼–ç å™¨\n",
    "   - å®Œæ•´çš„å…ƒæ•°æ®\n",
    "   - ä½¿ç”¨ç¤ºä¾‹ä»£ç \n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥\n",
    "\n",
    "- åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²æœ€ä½³æ¨¡å‹\n",
    "- å®æ—¶é¢„æµ‹ç³»å¤–è¡Œæ˜Ÿå€™é€‰\n",
    "- æŒç»­æ¨¡å‹ä¼˜åŒ–å’Œæ›´æ–°\n",
    "\n",
    "---\n",
    "\n",
    "**å¼€å‘æ—¥æœŸ**: 2025å¹´10æœˆ  \n",
    "**ç¯å¢ƒ**: Google Colab  \n",
    "**æ¡†æ¶**: TensorFlow 2.15.0 + XGBoost 2.0.3  \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Kepler Exoplanet 3-Class Detection 2025",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
