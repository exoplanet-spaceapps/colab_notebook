{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Kepler 系外行星三分类检测系统 (2025)\n",
    "\n",
    "## 项目概述\n",
    "本 Notebook 实现了基于 NASA Kepler 任务数据的系外行星三分类检测系统：\n",
    "- **CONFIRMED**: 已确认的系外行星\n",
    "- **CANDIDATE**: 候选系外行星\n",
    "- **FALSE POSITIVE**: 假阳性\n",
    "\n",
    "## 模型架构\n",
    "1. Genesis CNN (深度学习)\n",
    "2. XGBoost (梯度提升)\n",
    "3. Random Forest (随机森林)\n",
    "4. Voting Ensemble (集成学习)\n",
    "\n",
    "## 运行环境\n",
    "- Google Colab (2025年10月兼容)\n",
    "- TensorFlow 2.15.0\n",
    "- Python 3.10+\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1"
   },
   "source": [
    "## 1. 环境设置与依赖安装\n",
    "\n",
    "安装所有必需的库，确保与 2025 年 10 月环境兼容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# 安装依赖库（2025年10月兼容版本）\n",
    "!pip install -q tensorflow==2.15.0 xgboost==2.0.3 scikit-learn==1.3.2 \\\n",
    "    pandas==2.1.4 numpy==1.24.3 matplotlib==3.8.2 seaborn==0.13.0 \\\n",
    "    imbalanced-learn==0.11.0 joblib==1.3.2\n",
    "\n",
    "print(\"✅ 所有依赖库安装完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_curve, auc,\n",
    "    accuracy_score, precision_recall_fscore_support\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import joblib\n",
    "from google.colab import files\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置随机种子\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# 设置绘图样式\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"TensorFlow 版本: {tf.__version__}\")\n",
    "print(f\"XGBoost 版本: {xgb.__version__}\")\n",
    "print(f\"✅ 所有库导入成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2"
   },
   "source": [
    "## 2. 数据上传与加载\n",
    "\n",
    "上传 Kepler 数据集 CSV 文件并进行初步检查。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_data"
   },
   "outputs": [],
   "source": [
    "# 上传数据文件\n",
    "print(\"📤 请上传 Kepler 数据集 CSV 文件...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# 获取上传的文件名\n",
    "csv_filename = list(uploaded.keys())[0]\n",
    "print(f\"✅ 文件已上传: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "print(f\"数据集形状: {df.shape}\")\n",
    "print(f\"\\n列名: {df.columns.tolist()}\")\n",
    "print(f\"\\n前 5 行数据:\")\n",
    "display(df.head())\n",
    "\n",
    "# 检查标签分布\n",
    "if 'koi_disposition' in df.columns:\n",
    "    print(f\"\\n标签分布:\")\n",
    "    print(df['koi_disposition'].value_counts())\n",
    "    print(f\"\\n标签分布比例:\")\n",
    "    print(df['koi_disposition'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section3"
   },
   "source": [
    "## 3. 数据预处理\n",
    "\n",
    "清洗数据、处理缺失值、特征工程和数据平衡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocessing_functions"
   },
   "outputs": [],
   "source": [
    "def preprocess_kepler_data(df, test_size=0.2, balance_data=True):\n",
    "    \"\"\"\n",
    "    预处理 Kepler 数据集\n",
    "    \n",
    "    参数:\n",
    "        df: 原始数据框\n",
    "        test_size: 测试集比例\n",
    "        balance_data: 是否使用 SMOTE 平衡数据\n",
    "    \n",
    "    返回:\n",
    "        X_train, X_test, y_train, y_test, label_encoder, scaler\n",
    "    \"\"\"\n",
    "    print(\"🔄 开始数据预处理...\")\n",
    "    \n",
    "    # 1. 复制数据\n",
    "    data = df.copy()\n",
    "    \n",
    "    # 2. 处理标签列\n",
    "    label_col = 'koi_disposition'\n",
    "    if label_col not in data.columns:\n",
    "        raise ValueError(f\"数据集中未找到标签列 '{label_col}'\")\n",
    "    \n",
    "    # 3. 删除不相关的列\n",
    "    cols_to_drop = ['kepid', 'kepoi_name', 'kepler_name', 'koi_tce_delivname']\n",
    "    cols_to_drop = [col for col in cols_to_drop if col in data.columns]\n",
    "    data = data.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # 4. 分离特征和标签\n",
    "    X = data.drop(columns=[label_col])\n",
    "    y = data[label_col]\n",
    "    \n",
    "    # 5. 处理缺失值\n",
    "    print(f\"缺失值数量: {X.isnull().sum().sum()}\")\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    # 6. 标签编码\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    print(f\"\\n标签映射: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "    \n",
    "    # 7. 划分训练集和测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=test_size, random_state=RANDOM_SEED, stratify=y_encoded\n",
    "    )\n",
    "    print(f\"\\n训练集大小: {X_train.shape}\")\n",
    "    print(f\"测试集大小: {X_test.shape}\")\n",
    "    \n",
    "    # 8. 特征标准化\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 9. 数据平衡（SMOTE）\n",
    "    if balance_data:\n",
    "        print(\"\\n🔄 使用 SMOTE 进行数据平衡...\")\n",
    "        smote = SMOTE(random_state=RANDOM_SEED)\n",
    "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "        print(f\"平衡后训练集大小: {X_train_balanced.shape}\")\n",
    "        print(f\"平衡后标签分布: {np.bincount(y_train_balanced)}\")\n",
    "        return X_train_balanced, X_test_scaled, y_train_balanced, y_test, label_encoder, scaler\n",
    "    else:\n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test, label_encoder, scaler\n",
    "\n",
    "# 执行预处理\n",
    "X_train, X_test, y_train, y_test, label_encoder, scaler = preprocess_kepler_data(df)\n",
    "\n",
    "print(f\"\\n✅ 数据预处理完成！\")\n",
    "print(f\"特征数量: {X_train.shape[1]}\")\n",
    "print(f\"类别数量: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section4"
   },
   "source": [
    "## 4. 模型定义\n",
    "\n",
    "定义所有分类模型架构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "genesis_cnn"
   },
   "outputs": [],
   "source": [
    "def build_genesis_cnn(input_dim, num_classes=3):\n",
    "    \"\"\"\n",
    "    构建 Genesis CNN 模型\n",
    "    \n",
    "    架构:\n",
    "        - 输入层\n",
    "        - 3个卷积块（Conv1D + BatchNorm + ReLU + Dropout）\n",
    "        - 全局平均池化\n",
    "        - 全连接层\n",
    "        - Softmax 输出层\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(input_dim, 1), name='input')\n",
    "    \n",
    "    # 卷积块 1\n",
    "    x = layers.Conv1D(64, 3, padding='same', name='conv1')(inputs)\n",
    "    x = layers.BatchNormalization(name='bn1')(x)\n",
    "    x = layers.Activation('relu', name='relu1')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout1')(x)\n",
    "    \n",
    "    # 卷积块 2\n",
    "    x = layers.Conv1D(128, 3, padding='same', name='conv2')(x)\n",
    "    x = layers.BatchNormalization(name='bn2')(x)\n",
    "    x = layers.Activation('relu', name='relu2')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout2')(x)\n",
    "    \n",
    "    # 卷积块 3\n",
    "    x = layers.Conv1D(256, 3, padding='same', name='conv3')(x)\n",
    "    x = layers.BatchNormalization(name='bn3')(x)\n",
    "    x = layers.Activation('relu', name='relu3')(x)\n",
    "    x = layers.Dropout(0.4, name='dropout3')(x)\n",
    "    \n",
    "    # 全局池化\n",
    "    x = layers.GlobalAveragePooling1D(name='global_pool')(x)\n",
    "    \n",
    "    # 全连接层\n",
    "    x = layers.Dense(128, activation='relu', name='dense1')(x)\n",
    "    x = layers.Dropout(0.5, name='dropout4')(x)\n",
    "    \n",
    "    # 输出层\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Genesis_CNN')\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 构建模型\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "genesis_cnn = build_genesis_cnn(num_features, num_classes)\n",
    "print(genesis_cnn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section5"
   },
   "source": [
    "## 5. 模型训练\n",
    "\n",
    "训练所有模型并保存最佳版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_genesis"
   },
   "outputs": [],
   "source": [
    "# 创建模型保存目录\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# 准备数据（CNN需要3D输入）\n",
    "X_train_cnn = X_train.reshape(-1, num_features, 1)\n",
    "X_test_cnn = X_test.reshape(-1, num_features, 1)\n",
    "\n",
    "# 设置回调函数\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1),\n",
    "    ModelCheckpoint('models/genesis_cnn_best.keras', save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "]\n",
    "\n",
    "# 训练 Genesis CNN\n",
    "print(\"🚀 开始训练 Genesis CNN...\")\n",
    "history_cnn = genesis_cnn.fit(\n",
    "    X_train_cnn, y_train,\n",
    "    validation_data=(X_test_cnn, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 保存最终模型\n",
    "genesis_cnn.save('models/genesis_cnn_final.keras')\n",
    "print(\"✅ Genesis CNN 训练完成并已保存！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_xgboost"
   },
   "outputs": [],
   "source": [
    "# 训练 XGBoost\n",
    "print(\"\\n🚀 开始训练 XGBoost...\")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='multi:softmax',\n",
    "    num_class=num_classes,\n",
    "    random_state=RANDOM_SEED,\n",
    "    eval_metric='mlogloss',\n",
    "    early_stopping_rounds=15\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(xgb_model, 'models/xgboost_model.pkl')\n",
    "print(\"✅ XGBoost 训练完成并已保存！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_rf"
   },
   "outputs": [],
   "source": [
    "# 训练 Random Forest\n",
    "print(\"\\n🚀 开始训练 Random Forest...\")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(rf_model, 'models/random_forest_model.pkl')\n",
    "print(\"✅ Random Forest 训练完成并已保存！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_ensemble"
   },
   "outputs": [],
   "source": [
    "# 训练 Voting Ensemble\n",
    "print(\"\\n🚀 开始训练 Voting Ensemble...\")\n",
    "\n",
    "# 创建一个包装器用于 Genesis CNN\n",
    "class CNNWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_reshaped = X.reshape(-1, X.shape[1], 1)\n",
    "        return np.argmax(self.model.predict(X_reshaped, verbose=0), axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_reshaped = X.reshape(-1, X.shape[1], 1)\n",
    "        return self.model.predict(X_reshaped, verbose=0)\n",
    "\n",
    "cnn_wrapper = CNNWrapper(genesis_cnn)\n",
    "\n",
    "# 创建集成模型\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('genesis_cnn', cnn_wrapper),\n",
    "        ('xgboost', xgb_model),\n",
    "        ('random_forest', rf_model)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# 保存集成模型\n",
    "joblib.dump(voting_clf, 'models/voting_ensemble_model.pkl')\n",
    "print(\"✅ Voting Ensemble 训练完成并已保存！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section6"
   },
   "source": [
    "## 6. 模型评估与可视化\n",
    "\n",
    "评估所有模型性能并进行可视化比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation_functions"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name, is_cnn=False):\n",
    "    \"\"\"\n",
    "    评估单个模型\n",
    "    \n",
    "    返回:\n",
    "        metrics: 包含准确率、精确率、召回率、F1分数的字典\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"评估模型: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 预测\n",
    "    if is_cnn:\n",
    "        X_test_input = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "        y_pred_proba = model.predict(X_test_input, verbose=0)\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_test)\n",
    "        else:\n",
    "            y_pred_proba = None\n",
    "    \n",
    "    # 计算指标\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n📊 整体性能:\")\n",
    "    print(f\"  准确率: {accuracy:.4f}\")\n",
    "    print(f\"  精确率: {precision:.4f}\")\n",
    "    print(f\"  召回率: {recall:.4f}\")\n",
    "    print(f\"  F1分数: {f1:.4f}\")\n",
    "    \n",
    "    # 分类报告\n",
    "    print(f\"\\n📋 详细分类报告:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "    \n",
    "    # 混淆矩阵\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(cm, model_name, class_names):\n",
    "    \"\"\"\n",
    "    绘制混淆矩阵\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'混淆矩阵 - {model_name}')\n",
    "    plt.ylabel('真实标签')\n",
    "    plt.xlabel('预测标签')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'models/{model_name}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curves(models_metrics, X_test, y_test, class_names):\n",
    "    \"\"\"\n",
    "    绘制多类别 ROC 曲线（One-vs-Rest）\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from itertools import cycle\n",
    "    \n",
    "    # 二值化标签\n",
    "    y_test_bin = label_binarize(y_test, classes=range(len(class_names)))\n",
    "    n_classes = len(class_names)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = cycle(['blue', 'red', 'green'])\n",
    "    \n",
    "    for idx, metrics in enumerate(models_metrics):\n",
    "        if metrics['y_pred_proba'] is not None:\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            for i, color, class_name in zip(range(n_classes), colors, class_names):\n",
    "                fpr, tpr, _ = roc_curve(y_test_bin[:, i], metrics['y_pred_proba'][:, i])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                \n",
    "                ax.plot(fpr, tpr, color=color, lw=2,\n",
    "                       label=f'{class_name} (AUC = {roc_auc:.2f})')\n",
    "            \n",
    "            ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "            ax.set_xlim([0.0, 1.0])\n",
    "            ax.set_ylim([0.0, 1.05])\n",
    "            ax.set_xlabel('假阳性率')\n",
    "            ax.set_ylabel('真阳性率')\n",
    "            ax.set_title(f'ROC 曲线 - {metrics[\"model_name\"]}')\n",
    "            ax.legend(loc='lower right')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_all"
   },
   "outputs": [],
   "source": [
    "# 评估所有模型\n",
    "all_metrics = []\n",
    "\n",
    "# Genesis CNN\n",
    "metrics_cnn = evaluate_model(genesis_cnn, X_test, y_test, 'Genesis CNN', is_cnn=True)\n",
    "all_metrics.append(metrics_cnn)\n",
    "plot_confusion_matrix(metrics_cnn['confusion_matrix'], 'Genesis_CNN', label_encoder.classes_)\n",
    "\n",
    "# XGBoost\n",
    "metrics_xgb = evaluate_model(xgb_model, X_test, y_test, 'XGBoost')\n",
    "all_metrics.append(metrics_xgb)\n",
    "plot_confusion_matrix(metrics_xgb['confusion_matrix'], 'XGBoost', label_encoder.classes_)\n",
    "\n",
    "# Random Forest\n",
    "metrics_rf = evaluate_model(rf_model, X_test, y_test, 'Random Forest')\n",
    "all_metrics.append(metrics_rf)\n",
    "plot_confusion_matrix(metrics_rf['confusion_matrix'], 'Random_Forest', label_encoder.classes_)\n",
    "\n",
    "# Voting Ensemble\n",
    "metrics_ensemble = evaluate_model(voting_clf, X_test, y_test, 'Voting Ensemble')\n",
    "all_metrics.append(metrics_ensemble)\n",
    "plot_confusion_matrix(metrics_ensemble['confusion_matrix'], 'Voting_Ensemble', label_encoder.classes_)\n",
    "\n",
    "# 绘制 ROC 曲线对比\n",
    "plot_roc_curves(all_metrics, X_test, y_test, label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare_models"
   },
   "outputs": [],
   "source": [
    "# 模型性能对比\n",
    "comparison_df = pd.DataFrame([{\n",
    "    '模型': m['model_name'],\n",
    "    '准确率': f\"{m['accuracy']:.4f}\",\n",
    "    '精确率': f\"{m['precision']:.4f}\",\n",
    "    '召回率': f\"{m['recall']:.4f}\",\n",
    "    'F1分数': f\"{m['f1']:.4f}\"\n",
    "} for m in all_metrics])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 模型性能对比\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df)\n",
    "\n",
    "# 可视化对比\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "metric_names = ['准确率', '精确率', '召回率', 'F1分数']\n",
    "\n",
    "for idx, (metric, name) in enumerate(zip(metrics_to_plot, metric_names)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    values = [m[metric] for m in all_metrics]\n",
    "    model_names = [m['model_name'] for m in all_metrics]\n",
    "    \n",
    "    bars = ax.bar(model_names, values, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])\n",
    "    ax.set_ylabel(name)\n",
    "    ax.set_title(f'{name}对比')\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('models/models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section7"
   },
   "source": [
    "## 7. 自动选择最佳模型\n",
    "\n",
    "基于综合评分自动选择性能最好的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "select_best"
   },
   "outputs": [],
   "source": [
    "def select_best_model(metrics_list, weights={'accuracy': 0.3, 'precision': 0.2, 'recall': 0.2, 'f1': 0.3}):\n",
    "    \"\"\"\n",
    "    基于加权评分选择最佳模型\n",
    "    \n",
    "    参数:\n",
    "        metrics_list: 所有模型的评估指标列表\n",
    "        weights: 各指标的权重\n",
    "    \n",
    "    返回:\n",
    "        best_model_name, best_metrics\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for metrics in metrics_list:\n",
    "        score = (\n",
    "            weights['accuracy'] * metrics['accuracy'] +\n",
    "            weights['precision'] * metrics['precision'] +\n",
    "            weights['recall'] * metrics['recall'] +\n",
    "            weights['f1'] * metrics['f1']\n",
    "        )\n",
    "        scores.append(score)\n",
    "    \n",
    "    best_idx = np.argmax(scores)\n",
    "    best_metrics = metrics_list[best_idx]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🏆 最佳模型选择\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for idx, (metrics, score) in enumerate(zip(metrics_list, scores)):\n",
    "        marker = \"👑\" if idx == best_idx else \"  \"\n",
    "        print(f\"{marker} {metrics['model_name']}: 综合得分 = {score:.4f}\")\n",
    "    \n",
    "    print(f\"\\n✨ 最佳模型: {best_metrics['model_name']}\")\n",
    "    print(f\"   准确率: {best_metrics['accuracy']:.4f}\")\n",
    "    print(f\"   精确率: {best_metrics['precision']:.4f}\")\n",
    "    print(f\"   召回率: {best_metrics['recall']:.4f}\")\n",
    "    print(f\"   F1分数: {best_metrics['f1']:.4f}\")\n",
    "    \n",
    "    return best_metrics['model_name'], best_metrics\n",
    "\n",
    "# 选择最佳模型\n",
    "best_model_name, best_metrics = select_best_model(all_metrics)\n",
    "\n",
    "# 保存最佳模型信息\n",
    "best_model_info = {\n",
    "    'model_name': best_model_name,\n",
    "    'metrics': {\n",
    "        'accuracy': float(best_metrics['accuracy']),\n",
    "        'precision': float(best_metrics['precision']),\n",
    "        'recall': float(best_metrics['recall']),\n",
    "        'f1': float(best_metrics['f1'])\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'num_classes': num_classes,\n",
    "    'class_names': label_encoder.classes_.tolist()\n",
    "}\n",
    "\n",
    "with open('models/best_model_info.json', 'w') as f:\n",
    "    json.dump(best_model_info, f, indent=2)\n",
    "\n",
    "print(\"\\n✅ 最佳模型信息已保存！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section8"
   },
   "source": [
    "## 8. 训练历史可视化（CNN）\n",
    "\n",
    "可视化 Genesis CNN 的训练过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_history"
   },
   "outputs": [],
   "source": [
    "# 绘制训练历史\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# 损失曲线\n",
    "axes[0].plot(history_cnn.history['loss'], label='训练损失', linewidth=2)\n",
    "axes[0].plot(history_cnn.history['val_loss'], label='验证损失', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('损失')\n",
    "axes[0].set_title('Genesis CNN 训练损失曲线')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 准确率曲线\n",
    "axes[1].plot(history_cnn.history['accuracy'], label='训练准确率', linewidth=2)\n",
    "axes[1].plot(history_cnn.history['val_accuracy'], label='验证准确率', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('准确率')\n",
    "axes[1].set_title('Genesis CNN 训练准确率曲线')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('models/genesis_cnn_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section9"
   },
   "source": [
    "## 9. 保存模型元数据\n",
    "\n",
    "保存所有必要的预处理器和模型配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_metadata"
   },
   "outputs": [],
   "source": [
    "# 保存预处理器\n",
    "joblib.dump(scaler, 'models/scaler.pkl')\n",
    "joblib.dump(label_encoder, 'models/label_encoder.pkl')\n",
    "\n",
    "# 保存所有模型的元数据\n",
    "metadata = {\n",
    "    'project': 'Kepler Exoplanet 3-Class Detection',\n",
    "    'version': '2.0',\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'num_features': num_features,\n",
    "    'num_classes': num_classes,\n",
    "    'class_names': label_encoder.classes_.tolist(),\n",
    "    'train_size': len(X_train),\n",
    "    'test_size': len(X_test),\n",
    "    'models': {\n",
    "        'genesis_cnn': {\n",
    "            'type': 'deep_learning',\n",
    "            'framework': 'tensorflow',\n",
    "            'file': 'genesis_cnn_final.keras',\n",
    "            'metrics': {\n",
    "                'accuracy': float(metrics_cnn['accuracy']),\n",
    "                'precision': float(metrics_cnn['precision']),\n",
    "                'recall': float(metrics_cnn['recall']),\n",
    "                'f1': float(metrics_cnn['f1'])\n",
    "            }\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'type': 'gradient_boosting',\n",
    "            'framework': 'xgboost',\n",
    "            'file': 'xgboost_model.pkl',\n",
    "            'metrics': {\n",
    "                'accuracy': float(metrics_xgb['accuracy']),\n",
    "                'precision': float(metrics_xgb['precision']),\n",
    "                'recall': float(metrics_xgb['recall']),\n",
    "                'f1': float(metrics_xgb['f1'])\n",
    "            }\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'type': 'ensemble',\n",
    "            'framework': 'sklearn',\n",
    "            'file': 'random_forest_model.pkl',\n",
    "            'metrics': {\n",
    "                'accuracy': float(metrics_rf['accuracy']),\n",
    "                'precision': float(metrics_rf['precision']),\n",
    "                'recall': float(metrics_rf['recall']),\n",
    "                'f1': float(metrics_rf['f1'])\n",
    "            }\n",
    "        },\n",
    "        'voting_ensemble': {\n",
    "            'type': 'ensemble',\n",
    "            'framework': 'sklearn',\n",
    "            'file': 'voting_ensemble_model.pkl',\n",
    "            'metrics': {\n",
    "                'accuracy': float(metrics_ensemble['accuracy']),\n",
    "                'precision': float(metrics_ensemble['precision']),\n",
    "                'recall': float(metrics_ensemble['recall']),\n",
    "                'f1': float(metrics_ensemble['f1'])\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'best_model': best_model_name,\n",
    "    'preprocessing': {\n",
    "        'scaler': 'StandardScaler',\n",
    "        'label_encoder': 'LabelEncoder',\n",
    "        'balance_method': 'SMOTE'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('models/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"✅ 元数据已保存！\")\n",
    "print(\"\\n📦 模型文件列表:\")\n",
    "for file in os.listdir('models'):\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section10"
   },
   "source": [
    "## 10. 打包并下载模型\n",
    "\n",
    "将所有模型文件打包成 ZIP 文件供下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "package_models"
   },
   "outputs": [],
   "source": [
    "# 创建 ZIP 文件\n",
    "zip_filename = 'kepler_exoplanet_models_2025.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk('models'):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(file_path, '.')\n",
    "            zipf.write(file_path, arcname)\n",
    "            print(f\"✅ 已添加: {arcname}\")\n",
    "\n",
    "print(f\"\\n📦 所有模型已打包到: {zip_filename}\")\n",
    "print(f\"文件大小: {os.path.getsize(zip_filename) / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_models"
   },
   "outputs": [],
   "source": [
    "# 下载 ZIP 文件\n",
    "print(\"📥 开始下载模型文件...\")\n",
    "files.download(zip_filename)\n",
    "print(\"✅ 下载完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section11"
   },
   "source": [
    "## 11. 使用示例\n",
    "\n",
    "演示如何加载模型并进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usage_example"
   },
   "outputs": [],
   "source": [
    "# 示例：加载最佳模型并预测\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🔮 预测示例\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 加载元数据\n",
    "with open('models/metadata.json', 'r') as f:\n",
    "    loaded_metadata = json.load(f)\n",
    "\n",
    "print(f\"\\n最佳模型: {loaded_metadata['best_model']}\")\n",
    "\n",
    "# 加载预处理器\n",
    "loaded_scaler = joblib.load('models/scaler.pkl')\n",
    "loaded_label_encoder = joblib.load('models/label_encoder.pkl')\n",
    "\n",
    "# 选择几个测试样本\n",
    "sample_indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "X_sample = X_test[sample_indices]\n",
    "y_sample_true = y_test[sample_indices]\n",
    "\n",
    "# 根据最佳模型进行预测\n",
    "if best_model_name == 'Genesis CNN':\n",
    "    X_sample_input = X_sample.reshape(-1, num_features, 1)\n",
    "    y_sample_pred_proba = genesis_cnn.predict(X_sample_input, verbose=0)\n",
    "    y_sample_pred = np.argmax(y_sample_pred_proba, axis=1)\n",
    "elif best_model_name == 'XGBoost':\n",
    "    y_sample_pred = xgb_model.predict(X_sample)\n",
    "    y_sample_pred_proba = xgb_model.predict_proba(X_sample)\n",
    "elif best_model_name == 'Random Forest':\n",
    "    y_sample_pred = rf_model.predict(X_sample)\n",
    "    y_sample_pred_proba = rf_model.predict_proba(X_sample)\n",
    "else:  # Voting Ensemble\n",
    "    y_sample_pred = voting_clf.predict(X_sample)\n",
    "    y_sample_pred_proba = voting_clf.predict_proba(X_sample)\n",
    "\n",
    "# 显示预测结果\n",
    "print(\"\\n预测结果:\")\n",
    "for i in range(len(sample_indices)):\n",
    "    true_label = loaded_label_encoder.classes_[y_sample_true[i]]\n",
    "    pred_label = loaded_label_encoder.classes_[y_sample_pred[i]]\n",
    "    confidence = y_sample_pred_proba[i][y_sample_pred[i]] * 100\n",
    "    \n",
    "    match = \"✓\" if y_sample_true[i] == y_sample_pred[i] else \"✗\"\n",
    "    print(f\"\\n样本 {i+1}: {match}\")\n",
    "    print(f\"  真实标签: {true_label}\")\n",
    "    print(f\"  预测标签: {pred_label}\")\n",
    "    print(f\"  置信度: {confidence:.2f}%\")\n",
    "    print(f\"  概率分布: {dict(zip(loaded_label_encoder.classes_, [f'{p*100:.2f}%' for p in y_sample_pred_proba[i]]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## 总结\n",
    "\n",
    "### 📊 项目成果\n",
    "\n",
    "本 Notebook 成功实现了 Kepler 系外行星三分类检测系统，包括：\n",
    "\n",
    "1. **数据处理**\n",
    "   - 自动数据清洗和特征工程\n",
    "   - SMOTE 数据平衡\n",
    "   - 标准化预处理\n",
    "\n",
    "2. **模型训练**\n",
    "   - Genesis CNN（深度学习）\n",
    "   - XGBoost（梯度提升）\n",
    "   - Random Forest（随机森林）\n",
    "   - Voting Ensemble（集成学习）\n",
    "\n",
    "3. **性能评估**\n",
    "   - 混淆矩阵可视化\n",
    "   - ROC 曲线分析\n",
    "   - 综合指标对比\n",
    "   - 自动最佳模型选择\n",
    "\n",
    "4. **可部署资源**\n",
    "   - 所有训练好的模型\n",
    "   - 预处理器和编码器\n",
    "   - 完整的元数据\n",
    "   - 使用示例代码\n",
    "\n",
    "### 🚀 下一步\n",
    "\n",
    "- 在生产环境中部署最佳模型\n",
    "- 实时预测系外行星候选\n",
    "- 持续模型优化和更新\n",
    "\n",
    "---\n",
    "\n",
    "**开发日期**: 2025年10月  \n",
    "**环境**: Google Colab  \n",
    "**框架**: TensorFlow 2.15.0 + XGBoost 2.0.3  \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Kepler Exoplanet 3-Class Detection 2025",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
