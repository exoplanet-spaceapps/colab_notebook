# ğŸš€ Google Colab å¿«é€Ÿé–‹å§‹æŒ‡å—

## æœ€å¿«é€Ÿçš„ä½¿ç”¨æ–¹æ³•ï¼ˆ3æ­¥é©Ÿï¼‰

### Step 1: é–‹å•Ÿ Google Colab
å‰å¾€ https://colab.research.google.com/

### Step 2: è¤‡è£½è…³æœ¬åˆ° Colab
1. é»æ“Š "æ–°å¢ç­†è¨˜æœ¬"
2. åœ¨ç¨‹å¼ç¢¼å€å¡Šä¸­ï¼Œé»æ“Šå·¦ä¸Šè§’ "æª”æ¡ˆ" â†’ "ä¸Šå‚³ç­†è¨˜æœ¬"
3. æˆ–ç›´æ¥è¤‡è£½ `kepler_data_preprocessing_2025.py` çš„å…¨éƒ¨å…§å®¹åˆ°ç¨‹å¼ç¢¼å€å¡Š

### Step 3: åŸ·è¡Œï¼
é»æ“Šæ’­æ”¾æŒ‰éˆ• â–¶ï¸ æˆ–æŒ‰ `Ctrl+Enter`

---

## ğŸ“‹ åœ¨ Colab ä¸­çš„å®Œæ•´æ“ä½œæµç¨‹

### 1ï¸âƒ£ å‰µå»ºæ–°çš„ Colab Notebook

```python
# åœ¨ç¬¬ä¸€å€‹ç¨‹å¼ç¢¼å€å¡Šä¸­ï¼Œè¤‡è£½æ•´å€‹ kepler_data_preprocessing_2025.py æª”æ¡ˆå…§å®¹
# ç„¶å¾ŒåŸ·è¡Œé€™å€‹å€å¡Š
```

### 2ï¸âƒ£ ä¸Šå‚³æ•¸æ“šæª”æ¡ˆ

è…³æœ¬åŸ·è¡Œæ™‚æœƒè‡ªå‹•æç¤ºï¼š

```
ğŸ“¤ è«‹ä¸Šå‚³ features æª”æ¡ˆ...
```

**æ“ä½œæ­¥é©Ÿï¼š**
1. é»æ“Š "Choose Files" æŒ‰éˆ•
2. é¸æ“‡ `koi_lightcurve_features_no_label.csv`
3. ç­‰å¾…ä¸Šå‚³å®Œæˆ

ç„¶å¾Œæœƒæç¤ºï¼š

```
ğŸ“¤ è«‹ä¸Šå‚³ labels æª”æ¡ˆ...
```

**æ“ä½œæ­¥é©Ÿï¼š**
1. é»æ“Š "Choose Files" æŒ‰éˆ•
2. é¸æ“‡ `q1_q17_dr25_koi.csv`
3. ç­‰å¾…ä¸Šå‚³å®Œæˆ

### 3ï¸âƒ£ ç­‰å¾…è™•ç†å®Œæˆ

è…³æœ¬æœƒä¾åºåŸ·è¡Œï¼š
- âœ… [1/8] æª¢æŸ¥ç’°å¢ƒ
- âœ… [2/8] è¼‰å…¥æ•¸æ“š
- âœ… [3/8] æ•¸æ“šé©—è­‰
- âœ… [4/8] One-hot ç·¨ç¢¼
- âœ… [5/8] åˆä½µæ•¸æ“š
- âœ… [6/8] éš¨æ©Ÿæ‰“æ•£
- âœ… [7/8] è¨“ç·´/æ¸¬è©¦é›†åˆ‡åˆ†
- âœ… [8/8] è¦–è¦ºåŒ–

### 4ï¸âƒ£ ä¸‹è¼‰çµæœï¼ˆå¯é¸ï¼‰

å®Œæˆå¾Œæœƒçœ‹åˆ°æç¤ºï¼š

```
ğŸ’¾ æ˜¯å¦ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“šï¼Ÿ
è¼¸å…¥ 'y' ä¿å­˜æ•¸æ“šä¸¦ä¸‹è¼‰ï¼Œå…¶ä»–éµè·³é:
```

è¼¸å…¥ `y` ä¸¦æŒ‰ Enterï¼Œæœƒè‡ªå‹•ä¸‹è¼‰ï¼š
- `X_train.csv`
- `y_train.csv`
- `X_test.csv`
- `y_test.csv`
- `kepler_preprocessing_visualization.png`

---

## ğŸ¯ åŸ·è¡Œå¾Œçš„è®Šæ•¸

è™•ç†å®Œæˆå¾Œï¼Œä»¥ä¸‹è®Šæ•¸å¯ç›´æ¥åœ¨ Colab ä¸­ä½¿ç”¨ï¼š

```python
# è¨“ç·´é›†
X_train  # shape: (ç´„1400, 3197)
y_train  # shape: (ç´„1400, 3) - one-hot encoded

# æ¸¬è©¦é›†
X_test   # shape: (ç´„467, 3197)
y_test   # shape: (ç´„467, 3) - one-hot encoded
```

### æŸ¥çœ‹è®Šæ•¸è³‡è¨Š

```python
# åœ¨æ–°çš„ç¨‹å¼ç¢¼å€å¡Šä¸­åŸ·è¡Œ

print("è¨“ç·´é›†å½¢ç‹€:")
print(f"X_train: {X_train.shape}")
print(f"y_train: {y_train.shape}")

print("\næ¸¬è©¦é›†å½¢ç‹€:")
print(f"X_test: {X_test.shape}")
print(f"y_test: {y_test.shape}")

print("\næ¨™ç±¤æ¬„ä½:")
print(y_train.columns.tolist())
```

---

## ğŸ”§ å¸¸è¦‹æ“ä½œ

### æŸ¥çœ‹å‰å¹¾ç­†æ•¸æ“š

```python
# é¡¯ç¤ºå‰5ç­†è¨“ç·´æ•¸æ“š
print("X_train å‰5è¡Œ:")
print(X_train.head())

print("\ny_train å‰5è¡Œ:")
print(y_train.head())
```

### è½‰æ› one-hot ç‚ºå–®ä¸€æ¨™ç±¤

```python
# å°‡ one-hot ç·¨ç¢¼è½‰å›å–®ä¸€æ¨™ç±¤
y_train_labels = y_train.idxmax(axis=1)  # å›å‚³æ¬„ä½åç¨±
y_test_labels = y_test.idxmax(axis=1)

print("è½‰æ›å¾Œçš„æ¨™ç±¤ç¯„ä¾‹:")
print(y_train_labels.head())
```

### æª¢æŸ¥é¡åˆ¥åˆ†ä½ˆ

```python
# è¨“ç·´é›†é¡åˆ¥åˆ†ä½ˆ
print("è¨“ç·´é›†æ¨™ç±¤åˆ†ä½ˆ:")
for col in y_train.columns:
    count = y_train[col].sum()
    print(f"  {col}: {count} ({count/len(y_train)*100:.2f}%)")

# æ¸¬è©¦é›†é¡åˆ¥åˆ†ä½ˆ
print("\næ¸¬è©¦é›†æ¨™ç±¤åˆ†ä½ˆ:")
for col in y_test.columns:
    count = y_test[col].sum()
    print(f"  {col}: {count} ({count/len(y_test)*100:.2f}%)")
```

---

## ğŸ§ª å¿«é€Ÿæ¸¬è©¦æ¨¡å‹

å®Œæˆæ•¸æ“šè™•ç†å¾Œï¼Œå¯ä»¥ç«‹å³æ¸¬è©¦æ¨¡å‹ï¼š

### ç¯„ä¾‹ 1: Random Forest

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# è½‰æ›æ¨™ç±¤
y_train_labels = y_train.idxmax(axis=1)
y_test_labels = y_test.idxmax(axis=1)

# è¨“ç·´æ¨¡å‹
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train_labels)

# é æ¸¬èˆ‡è©•ä¼°
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test_labels, y_pred)

print(f"æº–ç¢ºç‡: {accuracy:.4f}")
print("\nåˆ†é¡å ±å‘Š:")
print(classification_report(y_test_labels, y_pred))
```

### ç¯„ä¾‹ 2: Logistic Regression

```python
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# æ¨™æº–åŒ–ç‰¹å¾µ
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# è½‰æ›æ¨™ç±¤
y_train_labels = y_train.idxmax(axis=1)
y_test_labels = y_test.idxmax(axis=1)

# è¨“ç·´æ¨¡å‹
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train_scaled, y_train_labels)

# è©•ä¼°
accuracy = model.score(X_test_scaled, y_test_labels)
print(f"æº–ç¢ºç‡: {accuracy:.4f}")
```

---

## ğŸ’¡ é€²éšæŠ€å·§

### 1. ä¿å­˜è®Šæ•¸ä¾›å¾ŒçºŒä½¿ç”¨

```python
# ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“šåˆ° Colab çš„æš«å­˜ç©ºé–“
import pickle

with open('processed_data.pkl', 'wb') as f:
    pickle.dump({
        'X_train': X_train,
        'y_train': y_train,
        'X_test': X_test,
        'y_test': y_test
    }, f)

print("âœ“ æ•¸æ“šå·²ä¿å­˜")
```

### 2. é‡æ–°è¼‰å…¥å·²ä¿å­˜çš„æ•¸æ“š

```python
import pickle

with open('processed_data.pkl', 'rb') as f:
    data = pickle.load(f)

X_train = data['X_train']
y_train = data['y_train']
X_test = data['X_test']
y_test = data['y_test']

print("âœ“ æ•¸æ“šå·²è¼‰å…¥")
```

### 3. é€£æ¥ Google Driveï¼ˆæŒä¹…åŒ–å­˜å„²ï¼‰

```python
from google.colab import drive
drive.mount('/content/drive')

# ä¿å­˜åˆ° Google Drive
X_train.to_csv('/content/drive/MyDrive/X_train.csv', index=False)
y_train.to_csv('/content/drive/MyDrive/y_train.csv', index=False)
X_test.to_csv('/content/drive/MyDrive/X_test.csv', index=False)
y_test.to_csv('/content/drive/MyDrive/y_test.csv', index=False)

print("âœ“ æ•¸æ“šå·²ä¿å­˜åˆ° Google Drive")
```

---

## ğŸ› ç–‘é›£æ’è§£

### å•é¡Œï¼šä¸Šå‚³æª”æ¡ˆå¤±æ•—

**è§£æ±ºæ–¹æ¡ˆï¼š**
```python
# æ‰‹å‹•ä¸Šå‚³æª”æ¡ˆçš„æ›¿ä»£æ–¹æ³•
from google.colab import files

print("ä¸Šå‚³ features æª”æ¡ˆ:")
uploaded = files.upload()
features_file = list(uploaded.keys())[0]

print("ä¸Šå‚³ labels æª”æ¡ˆ:")
uploaded = files.upload()
labels_file = list(uploaded.keys())[0]

# ç„¶å¾Œæ‰‹å‹•è¼‰å…¥
import pandas as pd
features = pd.read_csv(features_file)
labels = pd.read_csv(labels_file)
```

### å•é¡Œï¼šè¨˜æ†¶é«”ä¸è¶³

**è§£æ±ºæ–¹æ¡ˆï¼š**
```python
# å•Ÿç”¨é«˜è¨˜æ†¶é«”æ¨¡å¼ï¼ˆColab Pro å°ˆç”¨ï¼‰
# æˆ–æ¸›å°‘æ•¸æ“šç‰¹å¾µ

# é¸å–å‰1000å€‹ç‰¹å¾µ
X_train_reduced = X_train.iloc[:, :1000]
X_test_reduced = X_test.iloc[:, :1000]
```

### å•é¡Œï¼šåŸ·è¡Œæ™‚é–“éé•·

**è§£æ±ºæ–¹æ¡ˆï¼š**
```python
# ä½¿ç”¨ GPU åŠ é€Ÿï¼ˆå¦‚æœé©ç”¨ï¼‰
# æˆ–æ¸›å°‘æ•¸æ“šé‡é€²è¡Œå¿«é€Ÿæ¸¬è©¦

# å¿«é€Ÿæ¸¬è©¦ï¼šåªä½¿ç”¨10%çš„æ•¸æ“š
from sklearn.model_selection import train_test_split

X_sample, _, y_sample, _ = train_test_split(
    X_train, y_train,
    train_size=0.1,
    random_state=42
)
```

---

## âœ… æª¢æŸ¥æ¸…å–®

ä½¿ç”¨å‰è«‹ç¢ºèªï¼š

- [ ] å·²é–‹å•Ÿ Google Colab
- [ ] å·²è¤‡è£½å®Œæ•´è…³æœ¬
- [ ] æº–å‚™å¥½å…©å€‹ CSV æª”æ¡ˆ
- [ ] æª”æ¡ˆå¤§å°åˆç†ï¼ˆfeatures ç´„ 17MB, labels ç´„ 290KBï¼‰
- [ ] ç¶²è·¯é€£ç·šç©©å®š

---

## ğŸ“ å­¸ç¿’è³‡æº

### å»¶ä¼¸é–±è®€
- [Google Colab å®˜æ–¹æ–‡ä»¶](https://colab.research.google.com/notebooks/intro.ipynb)
- [pandas æ–‡ä»¶](https://pandas.pydata.org/docs/)
- [scikit-learn æ–‡ä»¶](https://scikit-learn.org/stable/)

### ç›¸é—œæ•™å­¸
- One-hot ç·¨ç¢¼: https://scikit-learn.org/stable/modules/preprocessing.html
- è¨“ç·´/æ¸¬è©¦é›†åˆ‡åˆ†: https://scikit-learn.org/stable/modules/cross_validation.html

---

## ğŸ‰ é–‹å§‹ä½¿ç”¨ï¼

ç¾åœ¨æ‚¨å·²ç¶“äº†è§£å¦‚ä½•åœ¨ Google Colab ä¸­ä½¿ç”¨é€™å€‹è…³æœ¬äº†ã€‚ç¥æ‚¨çš„æ©Ÿå™¨å­¸ç¿’å°ˆæ¡ˆé †åˆ©ï¼

**è¨˜ä½ï¼š** å®Œæˆå‰è™•ç†å¾Œï¼Œå°±å¯ä»¥é–‹å§‹è¨“ç·´å„ç¨®æ©Ÿå™¨å­¸ç¿’æ¨¡å‹äº†ï¼

---

**å¿«é€Ÿé€£çµï¼š**
- ğŸ“„ [å®Œæ•´ README](./README.md)
- ğŸ [ä¸»è…³æœ¬](./kepler_data_preprocessing_2025.py)
- ğŸŒ [Google Colab](https://colab.research.google.com/)
