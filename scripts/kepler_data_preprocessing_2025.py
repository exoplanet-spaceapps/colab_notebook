# -*- coding: utf-8 -*-
"""
Kepler Exoplanet è³‡æ–™å‰è™•ç†èˆ‡åˆ‡åˆ† - Google Colab 2025
=======================================================

æœ¬è…³æœ¬å°ˆç‚º Google Colab (2025å¹´10æœˆç’°å¢ƒ) è¨­è¨ˆ
- Python 3.11
- NumPy 2.0.2
- pandas (æœ€æ–°ç‰ˆæœ¬)
- scikit-learn (æœ€æ–°ç‰ˆæœ¬)

åŠŸèƒ½ï¼š
1. è¼‰å…¥ features å’Œ labels å…©å€‹ DataFrame
2. å¾ labels æå– koi_disposition ä¸¦è½‰æ›ç‚º one-hot ç·¨ç¢¼ï¼ˆä¸‰å€‹é¡åˆ¥ï¼‰
3. åˆä½µ features èˆ‡ one-hot labels
4. éš¨æ©Ÿæ‰“æ•£æ•¸æ“š
5. ä»¥ 3:1 æ¯”ä¾‹åˆ‡åˆ†è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†
6. è¼¸å‡º X_train, y_train, X_test, y_test

ä½œè€…ï¼šClaude AI
æ—¥æœŸï¼š2025-10-05
"""

# =============================================================================
# æ­¥é©Ÿ 1ï¼šç’°å¢ƒè¨­ç½®èˆ‡å¥—ä»¶æª¢æŸ¥
# =============================================================================

print("=" * 80)
print("ğŸš€ Kepler Exoplanet è³‡æ–™å‰è™•ç†è…³æœ¬å•Ÿå‹•")
print("=" * 80)
print("\n[1/8] æª¢æŸ¥ Python èˆ‡å¥—ä»¶ç‰ˆæœ¬...")

import sys
import warnings
warnings.filterwarnings('ignore')

# æª¢æŸ¥ Python ç‰ˆæœ¬
python_version = sys.version.split()[0]
print(f"  âœ“ Python ç‰ˆæœ¬: {python_version}")

# å°å…¥æ ¸å¿ƒå¥—ä»¶ä¸¦æª¢æŸ¥ç‰ˆæœ¬
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

print(f"  âœ“ NumPy ç‰ˆæœ¬: {np.__version__}")
print(f"  âœ“ pandas ç‰ˆæœ¬: {pd.__version__}")
print(f"  âœ“ scikit-learn ç‰ˆæœ¬: {sklearn.__version__}")

# è¨­å®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾æ€§
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

print(f"  âœ“ éš¨æ©Ÿç¨®å­å·²è¨­å®š: {RANDOM_STATE}")
print("\nâœ… ç’°å¢ƒæª¢æŸ¥å®Œæˆï¼\n")

# =============================================================================
# æ­¥é©Ÿ 2ï¼šæ•¸æ“šè¼‰å…¥
# =============================================================================

print("=" * 80)
print("[2/8] è¼‰å…¥æ•¸æ“šæª”æ¡ˆ...")
print("=" * 80)

# å¦‚æœåœ¨ Colab ä¸­ï¼Œéœ€è¦ä¸Šå‚³æª”æ¡ˆ
# åµæ¸¬æ˜¯å¦åœ¨ Colab ç’°å¢ƒ
try:
    import google.colab
    IN_COLAB = True
    print("\nğŸ” åµæ¸¬åˆ° Google Colab ç’°å¢ƒ")
    print("è«‹ä¸Šå‚³ä»¥ä¸‹å…©å€‹æª”æ¡ˆï¼š")
    print("  1. koi_lightcurve_features_no_label.csv (features)")
    print("  2. q1_q17_dr25_koi.csv (labels)")

    from google.colab import files

    print("\nğŸ“¤ è«‹ä¸Šå‚³ features æª”æ¡ˆ...")
    uploaded_features = files.upload()
    features_filename = list(uploaded_features.keys())[0]

    print("\nğŸ“¤ è«‹ä¸Šå‚³ labels æª”æ¡ˆ...")
    uploaded_labels = files.upload()
    labels_filename = list(uploaded_labels.keys())[0]

except ImportError:
    IN_COLAB = False
    print("\nğŸ” åµæ¸¬åˆ°æœ¬åœ°ç’°å¢ƒ")
    # æœ¬åœ°ç’°å¢ƒä½¿ç”¨ç›¸å°è·¯å¾‘
    features_filename = 'koi_lightcurve_features_no_label.csv'
    labels_filename = 'q1_q17_dr25_koi.csv'

# è¼‰å…¥æ•¸æ“š
print(f"\nğŸ“¥ è¼‰å…¥ features: {features_filename}")
features = pd.read_csv(features_filename)

print(f"ğŸ“¥ è¼‰å…¥ labels: {labels_filename}")
labels = pd.read_csv(labels_filename)

print(f"\n  âœ“ Features å½¢ç‹€: {features.shape} (rows, columns)")
print(f"  âœ“ Labels å½¢ç‹€: {labels.shape} (rows, columns)")

# é©—è­‰æ•¸æ“šè¡Œæ•¸æ˜¯å¦ä¸€è‡´
if len(features) != len(labels):
    print(f"\nâš ï¸ è­¦å‘Šï¼šfeatures ({len(features)}) èˆ‡ labels ({len(labels)}) è¡Œæ•¸ä¸ä¸€è‡´ï¼")
    print("æ­£åœ¨å°é½Šç´¢å¼•...")
    # å–äº¤é›†
    common_indices = features.index.intersection(labels.index)
    features = features.loc[common_indices]
    labels = labels.loc[common_indices]
    print(f"  âœ“ å°é½Šå¾Œæ•¸æ“šå½¢ç‹€: {features.shape}")

print("\nâœ… æ•¸æ“šè¼‰å…¥å®Œæˆï¼\n")

# =============================================================================
# æ­¥é©Ÿ 3ï¼šæ•¸æ“šæ¢ç´¢èˆ‡é©—è­‰
# =============================================================================

print("=" * 80)
print("[3/8] æ•¸æ“šæ¢ç´¢èˆ‡é©—è­‰...")
print("=" * 80)

# é¡¯ç¤º labels çš„æ¬„ä½
print("\nğŸ“‹ Labels DataFrame æ¬„ä½ï¼š")
print(labels.columns.tolist())

# æª¢æŸ¥ koi_disposition æ¬„ä½
if 'koi_disposition' in labels.columns:
    print("\nâœ“ æ‰¾åˆ° 'koi_disposition' æ¬„ä½")
    disposition_col = 'koi_disposition'
else:
    # å˜—è©¦æ‰¾åˆ°ç›¸ä¼¼çš„æ¬„ä½åç¨±
    possible_cols = [col for col in labels.columns if 'disposition' in col.lower()]
    if possible_cols:
        disposition_col = possible_cols[0]
        print(f"\nâš ï¸ æœªæ‰¾åˆ° 'koi_disposition'ï¼Œä½¿ç”¨ '{disposition_col}' æ›¿ä»£")
    else:
        raise ValueError("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° disposition ç›¸é—œæ¬„ä½ï¼")

# æå–æ¨™ç±¤
y = labels[disposition_col].copy()

# é¡¯ç¤ºæ¨™ç±¤åˆ†ä½ˆ
print(f"\nğŸ“Š æ¨™ç±¤ ('{disposition_col}') åˆ†ä½ˆï¼š")
print(y.value_counts())
print(f"\nå„é¡åˆ¥æ¯”ä¾‹ï¼š")
print(y.value_counts(normalize=True) * 100)

# æª¢æŸ¥ç¼ºå¤±å€¼
missing_labels = y.isnull().sum()
if missing_labels > 0:
    print(f"\nâš ï¸ è­¦å‘Šï¼šæ¨™ç±¤ä¸­æœ‰ {missing_labels} å€‹ç¼ºå¤±å€¼")
    print("æ­£åœ¨ç§»é™¤ç¼ºå¤±å€¼çš„æ¨£æœ¬...")
    valid_indices = y.notna()
    y = y[valid_indices]
    features = features[valid_indices]
    print(f"  âœ“ ç§»é™¤å¾Œæ•¸æ“šå½¢ç‹€: {features.shape}")

print("\nâœ… æ•¸æ“šé©—è­‰å®Œæˆï¼\n")

# =============================================================================
# æ­¥é©Ÿ 4ï¼šOne-Hot ç·¨ç¢¼
# =============================================================================

print("=" * 80)
print("[4/8] åŸ·è¡Œ One-Hot ç·¨ç¢¼...")
print("=" * 80)

# æ¨™æº–åŒ–æ¨™ç±¤å€¼ï¼ˆè½‰å°å¯«ä¸¦ç§»é™¤ç©ºæ ¼ï¼‰
y_normalized = y.str.strip().str.upper()

# é¡¯ç¤ºå”¯ä¸€å€¼
unique_values = y_normalized.unique()
print(f"\nå”¯ä¸€æ¨™ç±¤å€¼: {unique_values}")
print(f"æ¨™ç±¤é¡åˆ¥æ•¸: {len(unique_values)}")

# å°‡æ¨™ç±¤è½‰æ›ç‚º one-hot ç·¨ç¢¼
# ä½¿ç”¨ pandas get_dummies å‡½æ•¸
y_onehot = pd.get_dummies(y_normalized, prefix='label')

print(f"\n  âœ“ One-hot ç·¨ç¢¼å¾Œçš„å½¢ç‹€: {y_onehot.shape}")
print(f"  âœ“ One-hot æ¬„ä½: {y_onehot.columns.tolist()}")

# é¡¯ç¤ºå‰å¹¾è¡Œ
print("\nğŸ“‹ One-hot ç·¨ç¢¼ç¯„ä¾‹ï¼ˆå‰5è¡Œï¼‰ï¼š")
print(y_onehot.head())

# çµ±è¨ˆæ¯å€‹é¡åˆ¥çš„æ•¸é‡
print("\nğŸ“Š One-hot ç·¨ç¢¼å¾Œå„é¡åˆ¥æ•¸é‡ï¼š")
for col in y_onehot.columns:
    count = y_onehot[col].sum()
    print(f"  {col}: {count} ({count/len(y_onehot)*100:.2f}%)")

print("\nâœ… One-hot ç·¨ç¢¼å®Œæˆï¼\n")

# =============================================================================
# æ­¥é©Ÿ 5ï¼šåˆä½µ Features èˆ‡ Labels
# =============================================================================

print("=" * 80)
print("[5/8] åˆä½µ Features èˆ‡ One-hot Labels...")
print("=" * 80)

# é‡ç½®ç´¢å¼•ä»¥ç¢ºä¿å°é½Š
features_reset = features.reset_index(drop=True)
y_onehot_reset = y_onehot.reset_index(drop=True)

# åˆä½µæ•¸æ“š
combined_data = pd.concat([features_reset, y_onehot_reset], axis=1)

print(f"\n  âœ“ åˆä½µå¾Œæ•¸æ“šå½¢ç‹€: {combined_data.shape}")
print(f"  âœ“ ç¸½æ¬„ä½æ•¸: {combined_data.shape[1]}")
print(f"    - Features: {features.shape[1]}")
print(f"    - Labels: {y_onehot.shape[1]}")

# æª¢æŸ¥ç¼ºå¤±å€¼
missing_count = combined_data.isnull().sum().sum()
if missing_count > 0:
    print(f"\nâš ï¸ è­¦å‘Šï¼šåˆä½µå¾Œæ•¸æ“šæœ‰ {missing_count} å€‹ç¼ºå¤±å€¼")
    print("æ­£åœ¨è™•ç†ç¼ºå¤±å€¼...")
    # å¯é¸ï¼šå¡«å……æˆ–ç§»é™¤ç¼ºå¤±å€¼
    # combined_data = combined_data.dropna()
    # æˆ–è€…å¡«å……ç‚º 0
    combined_data = combined_data.fillna(0)
    print("  âœ“ ç¼ºå¤±å€¼å·²è™•ç†")

print("\nâœ… æ•¸æ“šåˆä½µå®Œæˆï¼\n")

# =============================================================================
# æ­¥é©Ÿ 6ï¼šéš¨æ©Ÿæ‰“æ•£ï¼ˆRandom Shuffleï¼‰
# =============================================================================

print("=" * 80)
print("[6/8] éš¨æ©Ÿæ‰“æ•£æ•¸æ“š...")
print("=" * 80)

print(f"\nä½¿ç”¨éš¨æ©Ÿç¨®å­: {RANDOM_STATE}")

# æ‰“æ•£æ•¸æ“š
shuffled_data = combined_data.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)

print(f"  âœ“ æ‰“æ•£å‰å½¢ç‹€: {combined_data.shape}")
print(f"  âœ“ æ‰“æ•£å¾Œå½¢ç‹€: {shuffled_data.shape}")

# é¡¯ç¤ºæ‰“æ•£å¾Œçš„å‰å¹¾è¡Œæ¨™ç±¤åˆ†ä½ˆ
label_cols = y_onehot.columns.tolist()
print("\nğŸ“‹ æ‰“æ•£å¾Œå‰10è¡Œçš„æ¨™ç±¤åˆ†ä½ˆï¼š")
print(shuffled_data[label_cols].head(10))

print("\nâœ… æ•¸æ“šæ‰“æ•£å®Œæˆï¼\n")

# =============================================================================
# æ­¥é©Ÿ 7ï¼šè¨“ç·´/æ¸¬è©¦é›†åˆ‡åˆ† (3:1 æ¯”ä¾‹)
# =============================================================================

print("=" * 80)
print("[7/8] åˆ‡åˆ†è¨“ç·´é›†èˆ‡æ¸¬è©¦é›† (3:1 æ¯”ä¾‹)...")
print("=" * 80)

# åˆ†é›¢ features å’Œ labels
X = shuffled_data.drop(columns=label_cols)
y = shuffled_data[label_cols]

print(f"\n  âœ“ X (features) å½¢ç‹€: {X.shape}")
print(f"  âœ“ y (labels) å½¢ç‹€: {y.shape}")

# ä½¿ç”¨ train_test_split åˆ‡åˆ†æ•¸æ“š
# test_size=0.25 è¡¨ç¤ºæ¸¬è©¦é›†ä½” 25% (1/4)ï¼Œè¨“ç·´é›†ä½” 75% (3/4)
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.25,
    random_state=RANDOM_STATE,
    stratify=y.idxmax(axis=1)  # ä¿æŒé¡åˆ¥æ¯”ä¾‹
)

print(f"\nğŸ“Š è¨“ç·´é›†å¤§å°:")
print(f"  âœ“ X_train: {X_train.shape}")
print(f"  âœ“ y_train: {y_train.shape}")

print(f"\nğŸ“Š æ¸¬è©¦é›†å¤§å°:")
print(f"  âœ“ X_test: {X_test.shape}")
print(f"  âœ“ y_test: {y_test.shape}")

# é©—è­‰æ¯”ä¾‹
total_samples = len(X)
train_samples = len(X_train)
test_samples = len(X_test)

print(f"\nğŸ“ˆ æ•¸æ“šåˆ‡åˆ†æ¯”ä¾‹é©—è­‰:")
print(f"  ç¸½æ¨£æœ¬æ•¸: {total_samples}")
print(f"  è¨“ç·´é›†: {train_samples} ({train_samples/total_samples*100:.2f}%)")
print(f"  æ¸¬è©¦é›†: {test_samples} ({test_samples/total_samples*100:.2f}%)")
print(f"  æ¯”ä¾‹: {train_samples/test_samples:.2f}:1")

# é¡¯ç¤ºå„é¡åˆ¥åœ¨è¨“ç·´é›†å’Œæ¸¬è©¦é›†ä¸­çš„åˆ†ä½ˆ
print("\nğŸ“Š è¨“ç·´é›†æ¨™ç±¤åˆ†ä½ˆ:")
for col in label_cols:
    count = y_train[col].sum()
    print(f"  {col}: {count} ({count/len(y_train)*100:.2f}%)")

print("\nğŸ“Š æ¸¬è©¦é›†æ¨™ç±¤åˆ†ä½ˆ:")
for col in label_cols:
    count = y_test[col].sum()
    print(f"  {col}: {count} ({count/len(y_test)*100:.2f}%)")

print("\nâœ… è¨“ç·´/æ¸¬è©¦é›†åˆ‡åˆ†å®Œæˆï¼\n")

# =============================================================================
# æ­¥é©Ÿ 8ï¼šè¦–è¦ºåŒ–èˆ‡çµæœæ‘˜è¦
# =============================================================================

print("=" * 80)
print("[8/8] ç”Ÿæˆè¦–è¦ºåŒ–èˆ‡çµæœæ‘˜è¦...")
print("=" * 80)

# è¨­å®šè¦–è¦ºåŒ–é¢¨æ ¼
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# å‰µå»ºè¦–è¦ºåŒ–åœ–è¡¨
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('Kepler Exoplanet è³‡æ–™å‰è™•ç†çµæœè¦–è¦ºåŒ–',
             fontsize=18, fontweight='bold', y=0.995)

# åœ–1: é¡åˆ¥åˆ†ä½ˆï¼ˆåŸå§‹æ•¸æ“šï¼‰
ax1 = axes[0, 0]
y_normalized.value_counts().plot(kind='bar', ax=ax1, color='skyblue', edgecolor='black')
ax1.set_title('åŸå§‹æ¨™ç±¤åˆ†ä½ˆ', fontsize=14, fontweight='bold')
ax1.set_xlabel('é¡åˆ¥', fontsize=12)
ax1.set_ylabel('æ•¸é‡', fontsize=12)
ax1.tick_params(axis='x', rotation=45)
for i, v in enumerate(y_normalized.value_counts()):
    ax1.text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')

# åœ–2: è¨“ç·´é›† vs æ¸¬è©¦é›†å¤§å°
ax2 = axes[0, 1]
sizes = [train_samples, test_samples]
labels_pie = [f'è¨“ç·´é›†\n{train_samples}\n({train_samples/total_samples*100:.1f}%)',
              f'æ¸¬è©¦é›†\n{test_samples}\n({test_samples/total_samples*100:.1f}%)']
colors_pie = ['#66b3ff', '#ff9999']
ax2.pie(sizes, labels=labels_pie, colors=colors_pie, autopct='',
        startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})
ax2.set_title('è¨“ç·´é›†/æ¸¬è©¦é›†æ¯”ä¾‹', fontsize=14, fontweight='bold')

# åœ–3: è¨“ç·´é›†æ¨™ç±¤åˆ†ä½ˆ
ax3 = axes[1, 0]
train_label_counts = y_train.sum()
train_label_counts.plot(kind='bar', ax=ax3, color='lightgreen', edgecolor='black')
ax3.set_title('è¨“ç·´é›†æ¨™ç±¤åˆ†ä½ˆ', fontsize=14, fontweight='bold')
ax3.set_xlabel('é¡åˆ¥', fontsize=12)
ax3.set_ylabel('æ•¸é‡', fontsize=12)
ax3.tick_params(axis='x', rotation=45)
for i, v in enumerate(train_label_counts):
    ax3.text(i, v + 20, str(int(v)), ha='center', va='bottom', fontweight='bold')

# åœ–4: æ¸¬è©¦é›†æ¨™ç±¤åˆ†ä½ˆ
ax4 = axes[1, 1]
test_label_counts = y_test.sum()
test_label_counts.plot(kind='bar', ax=ax4, color='lightcoral', edgecolor='black')
ax4.set_title('æ¸¬è©¦é›†æ¨™ç±¤åˆ†ä½ˆ', fontsize=14, fontweight='bold')
ax4.set_xlabel('é¡åˆ¥', fontsize=12)
ax4.set_ylabel('æ•¸é‡', fontsize=12)
ax4.tick_params(axis='x', rotation=45)
for i, v in enumerate(test_label_counts):
    ax4.text(i, v + 20, str(int(v)), ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.savefig('kepler_preprocessing_visualization.png', dpi=300, bbox_inches='tight')
print("\n  âœ“ è¦–è¦ºåŒ–åœ–è¡¨å·²ä¿å­˜: kepler_preprocessing_visualization.png")
plt.show()

# ç”Ÿæˆæ‘˜è¦å ±å‘Š
print("\n" + "=" * 80)
print("ğŸ“„ è³‡æ–™å‰è™•ç†æ‘˜è¦å ±å‘Š")
print("=" * 80)
print(f"""
âœ… è™•ç†å®Œæˆæ™‚é–“: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“Š æ•¸æ“šæ¦‚è¦½:
  â€¢ åŸå§‹æ•¸æ“šç¸½æ•¸: {total_samples:,} ç­†
  â€¢ Features ç¶­åº¦: {X.shape[1]:,} å€‹ç‰¹å¾µ
  â€¢ Labels é¡åˆ¥æ•¸: {len(label_cols)} é¡

ğŸ¯ æ¨™ç±¤é¡åˆ¥:
""")
for col in label_cols:
    original_count = y_onehot[col].sum()
    print(f"  â€¢ {col}: {original_count} ({original_count/len(y_onehot)*100:.2f}%)")

print(f"""
âœ‚ï¸ æ•¸æ“šåˆ‡åˆ†çµæœ:
  â€¢ è¨“ç·´é›†: {train_samples:,} ç­† ({train_samples/total_samples*100:.1f}%)
  â€¢ æ¸¬è©¦é›†: {test_samples:,} ç­† ({test_samples/total_samples*100:.1f}%)
  â€¢ åˆ‡åˆ†æ¯”ä¾‹: {train_samples/test_samples:.2f}:1

ğŸ”¢ è®Šæ•¸å½¢ç‹€:
  â€¢ X_train: {X_train.shape}
  â€¢ y_train: {y_train.shape}
  â€¢ X_test: {X_test.shape}
  â€¢ y_test: {y_test.shape}

ğŸŒ± éš¨æ©Ÿç¨®å­: {RANDOM_STATE}
""")

print("=" * 80)
print("\nğŸ‰ è³‡æ–™å‰è™•ç†æµç¨‹å…¨éƒ¨å®Œæˆï¼")
print("\nğŸ’¡ ä¸‹ä¸€æ­¥å¯ä»¥é–‹å§‹è¨“ç·´æ¨¡å‹ï¼š")
print("   â€¢ X_train, y_train ç”¨æ–¼è¨“ç·´")
print("   â€¢ X_test, y_test ç”¨æ–¼è©•ä¼°")
print("=" * 80)

# =============================================================================
# å¯é¸ï¼šä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š
# =============================================================================

print("\nğŸ’¾ æ˜¯å¦ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“šï¼Ÿ")

# å¦‚æœåœ¨ Colab ä¸­ï¼Œæä¾›ä¸‹è¼‰é¸é …
if IN_COLAB:
    save_data = input("è¼¸å…¥ 'y' ä¿å­˜æ•¸æ“šä¸¦ä¸‹è¼‰ï¼Œå…¶ä»–éµè·³é: ")
    if save_data.lower() == 'y':
        # ä¿å­˜ç‚º CSV
        X_train.to_csv('X_train.csv', index=False)
        y_train.to_csv('y_train.csv', index=False)
        X_test.to_csv('X_test.csv', index=False)
        y_test.to_csv('y_test.csv', index=False)

        print("\n  âœ“ æ•¸æ“šå·²ä¿å­˜ç‚º CSV æª”æ¡ˆ")

        # ä¸‹è¼‰æª”æ¡ˆ
        files.download('X_train.csv')
        files.download('y_train.csv')
        files.download('X_test.csv')
        files.download('y_test.csv')
        files.download('kepler_preprocessing_visualization.png')

        print("  âœ“ æª”æ¡ˆä¸‹è¼‰å®Œæˆ")
else:
    # æœ¬åœ°ç’°å¢ƒç›´æ¥ä¿å­˜
    X_train.to_csv('scripts/X_train.csv', index=False)
    y_train.to_csv('scripts/y_train.csv', index=False)
    X_test.to_csv('scripts/X_test.csv', index=False)
    y_test.to_csv('scripts/y_test.csv', index=False)
    print("\n  âœ“ æ•¸æ“šå·²ä¿å­˜åˆ° scripts/ ç›®éŒ„")

print("\n" + "=" * 80)
print("ğŸš€ è…³æœ¬åŸ·è¡Œå®Œç•¢ï¼æ„Ÿè¬ä½¿ç”¨ï¼")
print("=" * 80)
